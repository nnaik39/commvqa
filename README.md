# CommVQA: Situating Visual Question Answering in Communicative Contexts

This repository hosts all code and data for the paper CommVQA: Situating Visual Question Answering in Communicative Contexts (Arxiv, 2024).

This is the official Github repository for our paper Concadia: Towards Image-Based Text Generation with a Purpose. We provide the code and data necessary to replicate our results, as well as complimentary analyses.

## Downloading the CommVQA Dataset
CommVQA is a dataset introduced in our paper and contains Wikipedia images with their respective captions, alt descriptions and the broader context the images are situated in. We use this corpus to argue for a clear distinction between descriptions and captions, and show the similarities and differences between the two text forms. We further argue that captions and broader context are an important resource that can inform the generation of descriptions which are very sparse across the Web but absolutely crucial to make images accessible.

For more details, including how to download Concadia, navigate to Concadia_dataset/.

## Reproducing Section 4: Model Experiments
Please navigate to models/ for more details

## Citation
If you find this repo or the paper useful in your research, please feel free to cite our paper:

```
@inproceedings{Naik-etal:2024,
    author = {Naik, Nandita and Potts, Christopher and Kreiss, Elisa},
    booktitle={arXiv},
    year = {2024},
    title = {{CommVQA}: Situating {Visual Question Answering} in {Communicative Contexts}
```
